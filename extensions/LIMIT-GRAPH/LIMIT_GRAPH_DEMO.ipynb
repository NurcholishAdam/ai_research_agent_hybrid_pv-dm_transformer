{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIMIT-GRAPH v1.1: Multilingual Extension Demo\n",
    "\n",
    "This notebook demonstrates the multilingual capabilities of LIMIT-GRAPH v1.1, including:\n",
    "- Cross-lingual evaluation of retrieval and memory agents\n",
    "- Semantic graph structure preservation across languages\n",
    "- Entity linking and graph traversal in multilingual corpora\n",
    "\n",
    "## Supported Languages\n",
    "- Indonesian (id)\n",
    "- Spanish (es)\n",
    "- English (en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Load Multilingual Corpus and Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the agents directory to the path\n",
    "sys.path.append('agents')\n",
    "from multilingual_entity_linker import MultilingualEntityLinker, EntityMatch, GraphTraversalPath\n",
    "\n",
    "# Language selection - Change this to test different languages\n",
    "LANGUAGE = 'id'  # Options: 'id', 'es', 'en'\n",
    "\n",
    "print(f\"Selected language: {LANGUAGE}\")\n",
    "print(f\"Loading {LANGUAGE} dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line.strip()))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {filepath} not found\")\n",
    "    return data\n",
    "\n",
    "# Load multilingual datasets\n",
    "corpus = load_jsonl(f'data/corpus_{LANGUAGE}.jsonl')\n",
    "graph_edges = load_jsonl(f'data/graph_edges_{LANGUAGE}.jsonl')\n",
    "queries = load_jsonl(f'data/queries_{LANGUAGE}.jsonl')\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents\")\n",
    "print(f\"Loaded {len(graph_edges)} graph edges\")\n",
    "print(f\"Loaded {len(queries)} queries\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample corpus:\")\n",
    "for doc in corpus[:2]:\n",
    "    print(f\"  {doc['_id']}: {doc['text']}\")\n",
    "\n",
    "print(\"\\nSample graph edges:\")\n",
    "for edge in graph_edges[:3]:\n",
    "    print(f\"  {edge['source']} --[{edge['relation']}]--> {edge['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entity Linking - Run MultilingualEntityLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multilingual entity linker\n",
    "entity_linker = MultilingualEntityLinker(graph_edges, lang=LANGUAGE)\n",
    "\n",
    "# Process sample query\n",
    "sample_query = queries[0]['text'] if queries else \"Sample query\"\n",
    "print(f\"Processing query: {sample_query}\")\n",
    "\n",
    "# Run complete query processing\n",
    "result = entity_linker.process_query(sample_query)\n",
    "\n",
    "print(\"\\n=== Entity Linking Results ===\")\n",
    "print(f\"Extracted entities: {result['extracted_entities']}\")\n",
    "print(\"\\nLinked entities:\")\n",
    "for entity in result['linked_entities']:\n",
    "    print(f\"  '{entity['entity']}' -> '{entity['graph_node']}' (confidence: {entity['confidence']:.2f})\")\n",
    "    if entity['wikidata_qid']:\n",
    "        print(f\"    Wikidata QID: {entity['wikidata_qid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Traversal - Visualize Reasoning Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the semantic graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create NetworkX graph for visualization\n",
    "G = nx.DiGraph()\n",
    "for edge in graph_edges:\n",
    "    G.add_edge(edge['source'], edge['target'], relation=edge['relation'])\n",
    "\n",
    "# Layout and draw\n",
    "pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "        node_size=2000, font_size=10, font_weight='bold',\n",
    "        arrows=True, arrowsize=20, edge_color='gray')\n",
    "\n",
    "# Add edge labels\n",
    "edge_labels = {(edge['source'], edge['target']): edge['relation'] \n",
    "               for edge in graph_edges}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8)\n",
    "\n",
    "plt.title(f\"Semantic Graph - {LANGUAGE.upper()}\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display traversal paths\n",
    "print(\"\\n=== Graph Traversal Paths ===\")\n",
    "for i, path in enumerate(result['traversal_paths'][:3]):\n",
    "    print(f\"Path {i+1}: {' -> '.join(path['path'])}\")\n",
    "    print(f\"  Relations: {' -> '.join(path['relations'])}\")\n",
    "    print(f\"  Confidence: {path['confidence']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval Fusion - Sparse, Dense, and Graph Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate retrieval fusion results\n",
    "def simulate_retrieval_fusion(query, corpus, graph_paths):\n",
    "    \"\"\"Simulate retrieval fusion from sparse, dense, and graph modules\"\"\"\n",
    "    \n",
    "    # Sparse retrieval (keyword matching)\n",
    "    sparse_results = []\n",
    "    query_words = set(query.lower().split())\n",
    "    for doc in corpus:\n",
    "        doc_words = set(doc['text'].lower().split())\n",
    "        overlap = len(query_words.intersection(doc_words))\n",
    "        if overlap > 0:\n",
    "            sparse_results.append({\n",
    "                'doc_id': doc['_id'],\n",
    "                'text': doc['text'],\n",
    "                'score': overlap / len(query_words),\n",
    "                'method': 'sparse'\n",
    "            })\n",
    "    \n",
    "    # Dense retrieval (simulated semantic similarity)\n",
    "    dense_results = []\n",
    "    for doc in corpus:\n",
    "        # Simulate semantic similarity (in practice, use embeddings)\n",
    "        semantic_score = 0.8 if any(entity in doc['text'] for entity in result['extracted_entities']) else 0.3\n",
    "        dense_results.append({\n",
    "            'doc_id': doc['_id'],\n",
    "            'text': doc['text'],\n",
    "            'score': semantic_score,\n",
    "            'method': 'dense'\n",
    "        })\n",
    "    \n",
    "    # Graph retrieval (based on traversal paths)\n",
    "    graph_results = []\n",
    "    for path in graph_paths:\n",
    "        # Find documents containing path entities\n",
    "        for doc in corpus:\n",
    "            if any(node in doc['text'] for node in path['path']):\n",
    "                graph_results.append({\n",
    "                    'doc_id': doc['_id'],\n",
    "                    'text': doc['text'],\n",
    "                    'score': path['confidence'],\n",
    "                    'method': 'graph',\n",
    "                    'reasoning_path': ' -> '.join(path['path'])\n",
    "                })\n",
    "    \n",
    "    return sparse_results, dense_results, graph_results\n",
    "\n",
    "# Run retrieval fusion\n",
    "sparse_results, dense_results, graph_results = simulate_retrieval_fusion(\n",
    "    sample_query, corpus, result['traversal_paths']\n",
    ")\n",
    "\n",
    "print(\"=== Retrieval Fusion Results ===\")\n",
    "print(f\"\\nSparse Retrieval ({len(sparse_results)} results):\")\n",
    "for res in sparse_results:\n",
    "    print(f\"  {res['doc_id']}: {res['text']} (score: {res['score']:.3f})\")\n",
    "\n",
    "print(f\"\\nDense Retrieval ({len(dense_results)} results):\")\n",
    "for res in sorted(dense_results, key=lambda x: x['score'], reverse=True):\n",
    "    print(f\"  {res['doc_id']}: {res['text']} (score: {res['score']:.3f})\")\n",
    "\n",
    "print(f\"\\nGraph Retrieval ({len(graph_results)} results):\")\n",
    "for res in graph_results:\n",
    "    print(f\"  {res['doc_id']}: {res['text']} (score: {res['score']:.3f})\")\n",
    "    print(f\"    Reasoning path: {res['reasoning_path']}\")\n",
    "\n",
    "# RL Fusion Decision (simulated)\n",
    "print(\"\\n=== RL Fusion Decision ===\")\n",
    "fusion_weights = {'sparse': 0.3, 'dense': 0.4, 'graph': 0.3}\n",
    "print(f\"Fusion weights: {fusion_weights}\")\n",
    "print(\"Decision: Prioritize dense retrieval with graph reasoning support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Provenance Trace - Memory Lineage and Update History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Simulate provenance trace\n",
    "provenance_trace = {\n",
    "    'query_id': queries[0]['_id'] if queries else 'q1',\n",
    "    'timestamp': datetime.datetime.now().isoformat(),\n",
    "    'language': LANGUAGE,\n",
    "    'processing_steps': [\n",
    "        {\n",
    "            'step': 'entity_extraction',\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'input': sample_query,\n",
    "            'output': result['extracted_entities'],\n",
    "            'confidence': 0.95\n",
    "        },\n",
    "        {\n",
    "            'step': 'entity_linking',\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'input': result['extracted_entities'],\n",
    "            'output': [e['graph_node'] for e in result['linked_entities']],\n",
    "            'confidence': sum(e['confidence'] for e in result['linked_entities']) / len(result['linked_entities']) if result['linked_entities'] else 0\n",
    "        },\n",
    "        {\n",
    "            'step': 'graph_traversal',\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'input': [e['graph_node'] for e in result['linked_entities']],\n",
    "            'output': [p['path'] for p in result['traversal_paths'][:3]],\n",
    "            'confidence': sum(p['confidence'] for p in result['traversal_paths'][:3]) / min(3, len(result['traversal_paths'])) if result['traversal_paths'] else 0\n",
    "        }\n",
    "    ],\n",
    "    'memory_updates': [\n",
    "        {\n",
    "            'type': 'entity_cache',\n",
    "            'entities_added': result['extracted_entities'],\n",
    "            'timestamp': datetime.datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            'type': 'graph_cache',\n",
    "            'paths_cached': len(result['traversal_paths']),\n",
    "            'timestamp': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== Provenance Trace ===\")\n",
    "print(f\"Query ID: {provenance_trace['query_id']}\")\n",
    "print(f\"Language: {provenance_trace['language']}\")\n",
    "print(f\"Timestamp: {provenance_trace['timestamp']}\")\n",
    "\n",
    "print(\"\\nProcessing Steps:\")\n",
    "for i, step in enumerate(provenance_trace['processing_steps']):\n",
    "    print(f\"  {i+1}. {step['step']}\")\n",
    "    print(f\"     Input: {step['input']}\")\n",
    "    print(f\"     Output: {step['output']}\")\n",
    "    print(f\"     Confidence: {step['confidence']:.3f}\")\n",
    "\n",
    "print(\"\\nMemory Updates:\")\n",
    "for update in provenance_trace['memory_updates']:\n",
    "    print(f\"  - {update['type']}: {update}\")\n",
    "\n",
    "# Save provenance trace\n",
    "with open(f'provenance_trace_{LANGUAGE}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(provenance_trace, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\nProvenance trace saved to provenance_trace_{LANGUAGE}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation - Multilingual Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluation_metrics(results, graph_stats, provenance_trace):\n",
    "    \"\"\"Compute evaluation metrics for multilingual LIMIT-GRAPH\"\"\"\n",
    "    \n",
    "    # Recall@K (simulated)\n",
    "    k_values = [1, 3, 5]\n",
    "    recall_at_k = {}\n",
    "    \n",
    "    # Simulate ground truth relevance\n",
    "    relevant_docs = ['d12', 'd27']  # Assume these are relevant\n",
    "    retrieved_docs = [res['doc_id'] for res in sparse_results + dense_results]\n",
    "    \n",
    "    for k in k_values:\n",
    "        retrieved_k = retrieved_docs[:k]\n",
    "        relevant_retrieved = len(set(retrieved_k).intersection(set(relevant_docs)))\n",
    "        recall_at_k[f'recall@{k}'] = relevant_retrieved / len(relevant_docs) if relevant_docs else 0\n",
    "    \n",
    "    # Graph Coverage\n",
    "    total_nodes = graph_stats['node_count']\n",
    "    covered_nodes = len(set([e['graph_node'] for e in results['linked_entities']]))\n",
    "    graph_coverage = covered_nodes / total_nodes if total_nodes > 0 else 0\n",
    "    \n",
    "    # Provenance Integrity\n",
    "    processing_steps = len(provenance_trace['processing_steps'])\n",
    "    successful_steps = sum(1 for step in provenance_trace['processing_steps'] if step['confidence'] > 0.5)\n",
    "    provenance_integrity = successful_steps / processing_steps if processing_steps > 0 else 0\n",
    "    \n",
    "    # Entity Linking Accuracy\n",
    "    high_confidence_links = sum(1 for e in results['linked_entities'] if e['confidence'] > 0.8)\n",
    "    total_links = len(results['linked_entities'])\n",
    "    linking_accuracy = high_confidence_links / total_links if total_links > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'language': results['lang'],\n",
    "        'recall_at_k': recall_at_k,\n",
    "        'graph_coverage': graph_coverage,\n",
    "        'provenance_integrity': provenance_integrity,\n",
    "        'entity_linking_accuracy': linking_accuracy,\n",
    "        'graph_stats': graph_stats\n",
    "    }\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_evaluation_metrics(result, result['graph_stats'], provenance_trace)\n",
    "\n",
    "print(\"=== Evaluation Metrics ===\")\n",
    "print(f\"Language: {metrics['language']}\")\n",
    "print(f\"\\nRecall@K:\")\n",
    "for k, score in metrics['recall_at_k'].items():\n",
    "    print(f\"  {k}: {score:.3f}\")\n",
    "\n",
    "print(f\"\\nGraph Coverage: {metrics['graph_coverage']:.3f}\")\n",
    "print(f\"Provenance Integrity: {metrics['provenance_integrity']:.3f}\")\n",
    "print(f\"Entity Linking Accuracy: {metrics['entity_linking_accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\nGraph Statistics:\")\n",
    "for key, value in metrics['graph_stats'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save metrics\n",
    "with open(f'evaluation_metrics_{LANGUAGE}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\nMetrics saved to evaluation_metrics_{LANGUAGE}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Language Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare across languages (if multiple language files exist)\n",
    "languages_to_compare = ['id', 'es']\n",
    "comparison_results = {}\n",
    "\n",
    "for lang in languages_to_compare:\n",
    "    try:\n",
    "        # Load data for each language\n",
    "        lang_corpus = load_jsonl(f'data/corpus_{lang}.jsonl')\n",
    "        lang_edges = load_jsonl(f'data/graph_edges_{lang}.jsonl')\n",
    "        lang_queries = load_jsonl(f'data/queries_{lang}.jsonl')\n",
    "        \n",
    "        if lang_corpus and lang_edges and lang_queries:\n",
    "            # Initialize linker for this language\n",
    "            lang_linker = MultilingualEntityLinker(lang_edges, lang=lang)\n",
    "            \n",
    "            # Process query\n",
    "            lang_result = lang_linker.process_query(lang_queries[0]['text'])\n",
    "            \n",
    "            # Compute basic metrics\n",
    "            comparison_results[lang] = {\n",
    "                'entities_extracted': len(lang_result['extracted_entities']),\n",
    "                'entities_linked': len(lang_result['linked_entities']),\n",
    "                'traversal_paths': len(lang_result['traversal_paths']),\n",
    "                'avg_confidence': sum(e['confidence'] for e in lang_result['linked_entities']) / len(lang_result['linked_entities']) if lang_result['linked_entities'] else 0,\n",
    "                'graph_nodes': lang_result['graph_stats']['node_count'],\n",
    "                'graph_edges': lang_result['graph_stats']['edge_count']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process language {lang}: {e}\")\n",
    "\n",
    "# Display comparison\n",
    "if comparison_results:\n",
    "    print(\"=== Cross-Language Comparison ===\")\n",
    "    comparison_df = pd.DataFrame(comparison_results).T\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Entities comparison\n",
    "    axes[0, 0].bar(comparison_df.index, comparison_df['entities_extracted'])\n",
    "    axes[0, 0].set_title('Entities Extracted')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    axes[0, 1].bar(comparison_df.index, comparison_df['entities_linked'])\n",
    "    axes[0, 1].set_title('Entities Linked')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Confidence comparison\n",
    "    axes[1, 0].bar(comparison_df.index, comparison_df['avg_confidence'])\n",
    "    axes[1, 0].set_title('Average Linking Confidence')\n",
    "    axes[1, 0].set_ylabel('Confidence')\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Graph size comparison\n",
    "    axes[1, 1].bar(comparison_df.index, comparison_df['graph_nodes'], alpha=0.7, label='Nodes')\n",
    "    axes[1, 1].bar(comparison_df.index, comparison_df['graph_edges'], alpha=0.7, label='Edges')\n",
    "    axes[1, 1].set_title('Graph Size')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available. Make sure multiple language datasets exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== LIMIT-GRAPH v1.1 Demo Summary ===\")\n",
    "print(f\"Language processed: {LANGUAGE}\")\n",
    "print(f\"Query: {sample_query}\")\n",
    "print(f\"Entities extracted: {len(result['extracted_entities'])}\")\n",
    "print(f\"Entities linked: {len(result['linked_entities'])}\")\n",
    "print(f\"Traversal paths found: {len(result['traversal_paths'])}\")\n",
    "print(f\"Graph nodes: {result['graph_stats']['node_count']}\")\n",
    "print(f\"Graph edges: {result['graph_stats']['edge_count']}\")\n",
    "\n",
    "print(\"\\n=== Key Features Demonstrated ===\")\n",
    "print(\"âœ“ Multilingual entity extraction and linking\")\n",
    "print(\"âœ“ Cross-lingual semantic graph preservation\")\n",
    "print(\"âœ“ Graph traversal and reasoning path visualization\")\n",
    "print(\"âœ“ Retrieval fusion (sparse + dense + graph)\")\n",
    "print(\"âœ“ Provenance tracing and memory lineage\")\n",
    "print(\"âœ“ Comprehensive evaluation metrics\")\n",
    "print(\"âœ“ Cross-language performance comparison\")\n",
    "\n",
    "print(\"\\n=== Next Steps ===\")\n",
    "print(\"1. Test with different languages by changing LANGUAGE variable\")\n",
    "print(\"2. Add more complex queries and documents\")\n",
    "print(\"3. Implement real embedding-based dense retrieval\")\n",
    "print(\"4. Integrate with Wikidata for better entity alignment\")\n",
    "print(\"5. Add more sophisticated RL fusion strategies\")\n",
    "print(\"6. Extend evaluation with human judgments\")\n",
    "\n",
    "print(\"\\n=== Files Generated ===\")\n",
    "print(f\"- provenance_trace_{LANGUAGE}.json\")\n",
    "print(f\"- evaluation_metrics_{LANGUAGE}.json\")\n",
    "print(\"\\nDemo completed successfully! ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}